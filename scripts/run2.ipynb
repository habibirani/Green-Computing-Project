{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E8cHH8e9Cg1C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil  # https://docs.python.org/3/library/shutil.html\n",
        "from shutil import unpack_archive  # to unzip\n",
        "# from shutil import make_archive # to create zip for storage\n",
        "import requests  # for downloading zip file\n",
        "from scipy import io  # for loadmat, matlab conversion\n",
        "import numpy as np\n",
        "\n",
        "# credit https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
        "# many other methods I tried failed to download the file properly\n",
        "\n",
        "\n",
        "def download_url(url, save_path, chunk_size=128):\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(save_path, 'wb') as fd:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            fd.write(chunk)\n",
        "\n",
        "\n",
        "def unimib_load_dataset(\n",
        "        verbose=True,\n",
        "        incl_xyz_accel=False,  # include component accel_x/y/z in ____X data\n",
        "        # add rms value (total accel) of accel_x/y/z in ____X data\n",
        "        incl_rms_accel=True,\n",
        "        incl_val_group=False,  # True => returns x/y_test, x/y_validation, x/y_train\n",
        "    # False => combine test & validation groups\n",
        "        split_subj=dict\n",
        "    (train_subj=[4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 19, 20, 21, 22, 24, 26, 27, 29],\n",
        "     validation_subj=[1, 9, 16, 23, 25, 28],\n",
        "     test_subj=[2, 3, 13, 17, 18, 30]),\n",
        "        one_hot_encode=True):\n",
        "\n",
        "    # Download and unzip original dataset\n",
        "    if (not os.path.isfile('./UniMiB-SHAR.zip')):\n",
        "        print(\"Downloading UniMiB-SHAR.zip file\")\n",
        "        # invoking the shell command fails when exported to .py file\n",
        "        # redirect link https://www.dropbox.com/s/raw/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip\n",
        "        #!wget https://www.dropbox.com/s/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip\n",
        "        download_url(\n",
        "            'https://www.dropbox.com/s/raw/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip', './UniMiB-SHAR.zip')\n",
        "    if (not os.path.isdir('./UniMiB-SHAR')):\n",
        "        shutil.unpack_archive('./UniMiB-SHAR.zip', '.', 'zip')\n",
        "    # Convert .mat files to numpy ndarrays\n",
        "    path_in = './UniMiB-SHAR/data'\n",
        "    # loadmat loads matlab files as dictionary, keys: header, version, globals, data\n",
        "    adl_data = io.loadmat(path_in + '/adl_data.mat')['adl_data']\n",
        "    adl_names = io.loadmat(path_in + '/adl_names.mat',\n",
        "                           chars_as_strings=True)['adl_names']\n",
        "    adl_labels = io.loadmat(path_in + '/adl_labels.mat')['adl_labels']\n",
        "\n",
        "    # Reshape data and compute total (rms) acceleration\n",
        "    num_samples = 151\n",
        "    # UniMiB SHAR has fixed size of 453 which is 151 accelX, 151 accely, 151 accelz\n",
        "    adl_data = np.reshape(adl_data, (-1, num_samples, 3),\n",
        "                          order='F')  # uses Fortran order\n",
        "    if (incl_rms_accel):\n",
        "        rms_accel = np.sqrt(\n",
        "            (adl_data[:, :, 0]**2) + (adl_data[:, :, 1]**2) + (adl_data[:, :, 2]**2))\n",
        "        adl_data = np.dstack((adl_data, rms_accel))\n",
        "\n",
        "    # remove component accel if needed\n",
        "    if (not incl_xyz_accel):\n",
        "        adl_data = np.delete(adl_data, [0, 1, 2], 2)\n",
        "\n",
        "    # matlab source was 1 indexed, change to 0 indexed\n",
        "    act_num = (adl_labels[:, 0])-1\n",
        "    sub_num = (adl_labels[:, 1])  # subject numbers are in column 1 of labels\n",
        "\n",
        "    if (not incl_val_group):\n",
        "        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] +\n",
        "                                         split_subj['validation_subj']))\n",
        "        x_train = adl_data[train_index]\n",
        "        y_train = act_num[train_index]\n",
        "    else:\n",
        "        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj']))\n",
        "        x_train = adl_data[train_index]\n",
        "        y_train = act_num[train_index]\n",
        "\n",
        "        validation_index = np.nonzero(\n",
        "            np.isin(sub_num, split_subj['validation_subj']))\n",
        "        x_validation = adl_data[validation_index]\n",
        "        y_validation = act_num[validation_index]\n",
        "\n",
        "    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n",
        "    x_test = adl_data[test_index]\n",
        "    y_test = act_num[test_index]\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"x/y_train shape \", x_train.shape, y_train.shape)\n",
        "    if (incl_val_group):\n",
        "        print(\"x/y_validation shape \", x_validation.shape, y_validation.shape)\n",
        "    print(\"x/y_test shape  \", x_test.shape, y_test.shape)\n",
        "\n",
        "    if (incl_val_group):\n",
        "        return x_train, y_train, x_validation, y_validation, x_test, y_test\n",
        "    else:\n",
        "        return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_valid, y_valid, X_test, y_test = unimib_load_dataset(\n",
        "    incl_xyz_accel=True, incl_val_group=True, one_hot_encode=False, verbose=True\n",
        ")\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYSBW_gKCnu4",
        "outputId": "5e8cccec-3eb2-493e-b0b9-a417c355926c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading UniMiB-SHAR.zip file\n",
            "x/y_train shape  (4601, 151, 4) (4601,)\n",
            "x/y_validation shape  (1454, 151, 4) (1454,)\n",
            "x/y_test shape   (1524, 151, 4) (1524,)\n",
            "Number of classes: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuuSAE4DC7iS",
        "outputId": "8368fe08-408d-4d01-a1c6-6bd2cd05cda0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "import torch.quantization as quant\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "mZrgnYcXCuk5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "Y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "valid_dataset = TensorDataset(X_valid, Y_valid)\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, Y_test)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "4oKNfTFIDC4C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Calculate the number of patches, adjusting for padding if necessary\n",
        "        # Ceiling division to account for padding\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (\n",
        "            self.num_patches * patch_size\n",
        "        ) - input_timesteps  # Calculate padding length\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad the input sequence if necessary\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n",
        "\n",
        "        # We use a Conv1d layer to generate the patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "T6Job_-lDFDK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "summary(\n",
        "    model=patch_embedding_layer,\n",
        "    # (batch_size, input_channels, input_timesteps)\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfx_fefRDIxg",
        "outputId": "4066f9ea-4d1b-4810-bda9-cdf4287f8f27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 151, 4]         [64, 20, 32]         32                   True\n",
              "├─Conv1d (conv_layer)                                             [64, 4, 152]         [64, 32, 19]         1,056                True\n",
              "├─PositionalEncoding (position_embeddings)                        [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "│    └─Dropout (dropout)                                          [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "=================================================================================================================================================\n",
              "Total params: 1,088\n",
              "Trainable params: 1,088\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.28\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.15\n",
              "Forward/backward pass size (MB): 0.31\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.47\n",
              "================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        # Setting batch_first=True to accommodate inputs with batch dimension first\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Apply Transformer Encoder with batch first\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "jiv86iUbDL3e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=12,\n",
        "    num_heads=16,\n",
        "    dim_feedforward=256,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89MCYH4ZDOuT",
        "outputId": "80e32340-5064-42c7-d2fe-acd29b2d06fe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "TimeSeriesTransformer (TimeSeriesTransformer)           [64, 151, 4]         [64, 9]              --                   True\n",
              "├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 151, 4]         [64, 20, 32]         32                   True\n",
              "│    └─Conv1d (conv_layer)                              [64, 4, 152]         [64, 32, 19]         1,056                True\n",
              "│    └─PositionalEncoding (position_embeddings)         [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "│    │    └─Dropout (dropout)                           [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "├─TransformerEncoder (transformer_encoder)              [64, 20, 32]         [64, 20, 32]         --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─TransformerEncoderLayer (0)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (1)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (2)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (3)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (4)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (5)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (6)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (7)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (8)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (9)                 [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (10)                [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "│    │    └─TransformerEncoderLayer (11)                [64, 20, 32]         [64, 20, 32]         21,024               True\n",
              "├─Linear (ff_layer)                                     [64, 32]             [64, 256]            8,448                True\n",
              "├─Linear (classifier)                                   [64, 256]            [64, 9]              2,313                True\n",
              "=======================================================================================================================================\n",
              "Total params: 264,137\n",
              "Trainable params: 264,137\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.97\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.15\n",
              "Forward/backward pass size (MB): 0.45\n",
              "Params size (MB): 0.05\n",
              "Estimated Total Size (MB): 0.65\n",
              "======================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 10\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        predictions = model(inputs)  # Forward pass\n",
        "        loss = criterion(predictions, labels)  # Calculate loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Count the number of correct predictions\n",
        "        train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_acc = train_correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymqOWnnbDQ3k",
        "outputId": "9ce9c91f-ad5e-4482-da21-67bc3d566742"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.4169\n",
            "Epoch 1, Train Loss: 1.7402, Train Acc: 0.3261, Val Loss: 1.4402, Val Acc: 0.4169\n",
            "Epoch 2: New best model saved with validation accuracy: 0.4553\n",
            "Epoch 2, Train Loss: 1.2032, Train Acc: 0.5224, Val Loss: 1.2767, Val Acc: 0.4553\n",
            "Epoch 3: New best model saved with validation accuracy: 0.5362\n",
            "Epoch 3, Train Loss: 1.0577, Train Acc: 0.5742, Val Loss: 1.2460, Val Acc: 0.5362\n",
            "Epoch 4, Train Loss: 0.9686, Train Acc: 0.6059, Val Loss: 1.1803, Val Acc: 0.5341\n",
            "Epoch 5, Train Loss: 0.8911, Train Acc: 0.6402, Val Loss: 1.4010, Val Acc: 0.5156\n",
            "Epoch 6: New best model saved with validation accuracy: 0.5923\n",
            "Epoch 6, Train Loss: 0.8419, Train Acc: 0.6558, Val Loss: 1.1199, Val Acc: 0.5923\n",
            "Epoch 7: New best model saved with validation accuracy: 0.6236\n",
            "Epoch 7, Train Loss: 0.7550, Train Acc: 0.7082, Val Loss: 1.5353, Val Acc: 0.6236\n",
            "Epoch 8: New best model saved with validation accuracy: 0.6889\n",
            "Epoch 8, Train Loss: 0.7063, Train Acc: 0.7273, Val Loss: 1.0439, Val Acc: 0.6889\n",
            "Epoch 9: New best model saved with validation accuracy: 0.7038\n",
            "Epoch 9, Train Loss: 0.6048, Train Acc: 0.7689, Val Loss: 0.9022, Val Acc: 0.7038\n",
            "Epoch 10, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 1.1199, Val Acc: 0.6925\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "import time\n",
        "start_time = time.time()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "end_time = time.time()\n",
        "\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "confusion = confusion_matrix(Y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")\n",
        "print(f\"Original Model Average Inference Time: {end_time - start_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27-Xe3d2DdzD",
        "outputId": "93632196-52bb-4344-80b8-606874f8a56b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        34\n",
            "           1       0.88      0.15      0.25        47\n",
            "           2       0.55      0.99      0.71       344\n",
            "           3       0.95      0.80      0.87       413\n",
            "           4       0.56      0.27      0.36       184\n",
            "           5       0.86      0.49      0.63       146\n",
            "           6       0.55      0.44      0.49       256\n",
            "           7       0.18      0.16      0.17        68\n",
            "           8       0.23      0.84      0.36        32\n",
            "\n",
            "    accuracy                           0.62      1524\n",
            "   macro avg       0.53      0.46      0.43      1524\n",
            "weighted avg       0.67      0.62      0.60      1524\n",
            "\n",
            "Confusion matrix:\n",
            "[[  0   1   0   0   0   0   0  19  14]\n",
            " [  0   7   0   0   1   0   0  19  20]\n",
            " [  0   0 341   0   0   0   3   0   0]\n",
            " [  0   0   7 330   0  12  64   0   0]\n",
            " [  0   0 125   0  49   0   4   6   0]\n",
            " [  0   0   1  17  35  72  21   0   0]\n",
            " [  0   0 141   0   2   0 113   0   0]\n",
            " [  0   0   0   0   0   0   0  11  57]\n",
            " [  0   0   0   0   0   0   0   5  27]]\n",
            "Original Model Average Inference Time: 5.087586879730225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define quantization configuration\n",
        "quant_config = quant.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Apply post-training static quantization\n",
        "quant_model = quant.quantize_dynamic(\n",
        "    model, qconfig_spec={\"\": quant_config}, dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "gkNMKGQ_DtXG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 10\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "# Validation loop\n",
        "    quant_model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4dv31znEgrT",
        "outputId": "f5a8216e-5f04-4bd7-9ec4-745d53fac5d1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.6967\n",
            "Epoch 1, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8553, Val Acc: 0.6967\n",
            "Epoch 2: New best model saved with validation accuracy: 0.7010\n",
            "Epoch 2, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8513, Val Acc: 0.7010\n",
            "Epoch 3, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8521, Val Acc: 0.6953\n",
            "Epoch 4: New best model saved with validation accuracy: 0.7031\n",
            "Epoch 4, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8486, Val Acc: 0.7031\n",
            "Epoch 5: New best model saved with validation accuracy: 0.7060\n",
            "Epoch 5, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8369, Val Acc: 0.7060\n",
            "Epoch 6: New best model saved with validation accuracy: 0.7074\n",
            "Epoch 6, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8418, Val Acc: 0.7074\n",
            "Epoch 7, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8657, Val Acc: 0.7038\n",
            "Epoch 8, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8744, Val Acc: 0.6918\n",
            "Epoch 9, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8553, Val Acc: 0.6974\n",
            "Epoch 10, Train Loss: 0.6003, Train Acc: 0.7716, Val Loss: 0.8592, Val Acc: 0.6932\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "start_time = time.time()\n",
        "quant_model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "end_time = time.time()\n",
        "\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "confusion = confusion_matrix(Y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")\n",
        "print(f\"Quantized Model Average Inference Time: {end_time - start_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nagjMYNGFea6",
        "outputId": "55a06af2-830e-4cc0-9414-a57438bc3e15"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        34\n",
            "           1       0.78      0.15      0.25        47\n",
            "           2       0.66      0.95      0.78       344\n",
            "           3       0.93      0.80      0.86       413\n",
            "           4       0.57      0.39      0.46       184\n",
            "           5       0.71      0.41      0.52       146\n",
            "           6       0.59      0.65      0.62       256\n",
            "           7       0.33      0.40      0.36        68\n",
            "           8       0.23      0.66      0.34        32\n",
            "\n",
            "    accuracy                           0.66      1524\n",
            "   macro avg       0.53      0.49      0.47      1524\n",
            "weighted avg       0.68      0.66      0.65      1524\n",
            "\n",
            "Confusion matrix:\n",
            "[[  0   1   0   0   0   0   0  17  16]\n",
            " [  0   7   0   0   4   0   0  22  14]\n",
            " [  0   0 327   0   4   0  13   0   0]\n",
            " [  0   0   1 329   0  23  60   0   0]\n",
            " [  0   1  92   1  72   0  13   5   0]\n",
            " [  0   0   1  25  30  60  30   0   0]\n",
            " [  0   0  71   0  17   2 166   0   0]\n",
            " [  0   0   0   0   0   0   0  27  41]\n",
            " [  0   0   0   0   0   0   0  11  21]]\n",
            "Quantized Model Average Inference Time: 3.355748414993286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune the model\n",
        "prune_amount = 0.2\n",
        "prune_method = prune.L1Unstructured\n",
        "\n",
        "# Get the sequence length from the shape of your input data\n",
        "sequence_length = X_train.shape[1]\n",
        "\n",
        "# Define dummy input shape based on your data\n",
        "batch_size = X_train.shape[0]\n",
        "input_dim = X_train.shape[2]\n",
        "input_shape = (batch_size, sequence_length, input_dim)\n",
        "\n",
        "# Define the parameters to be pruned\n",
        "parameters_to_prune = [\n",
        "    (name, module) for name, module in model.named_parameters() if 'your_module_to_prune' in name\n",
        "]\n",
        "\n",
        "# Apply pruning to model\n",
        "for name, module in parameters_to_prune:\n",
        "    prune_method(module, amount=prune_amount)  # Prune based on L1 norm"
      ],
      "metadata": {
        "id": "ILMO0Kc8JDb7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pruning method and amount\n",
        "import copy\n",
        "\n",
        "# Create a copy of the original model\n",
        "pruned_model = copy.deepcopy(model)\n",
        "\n",
        "# Prune the model\n",
        "prune_amount = 0.2\n",
        "prune_method = prune.L1Unstructured\n",
        "\n",
        "# Get the sequence length from the shape of your input data\n",
        "sequence_length = X_train.shape[1]\n",
        "\n",
        "# Define dummy input shape based on your data\n",
        "batch_size = X_train.shape[0]\n",
        "input_dim = X_train.shape[2]\n",
        "input_shape = (batch_size, sequence_length, input_dim)\n",
        "\n",
        "# Define the parameters to be pruned\n",
        "parameters_to_prune = [\n",
        "    (name, module) for name, module in model.named_parameters() if 'your_module_to_prune' in name\n",
        "]\n",
        "\n",
        "# Apply pruning to model\n",
        "for name, module in parameters_to_prune:\n",
        "    prune_method(module, amount=prune_amount)  # Prune based on L1 norm"
      ],
      "metadata": {
        "id": "hTZb7J5K9A-Z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the pruned model\n",
        "start_time = time.time()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob_pruned = pruned_model(X_test)\n",
        "\n",
        "Y_pred_pruned = Y_pred_prob_pruned.argmax(1)\n",
        "end_time = time.time()\n",
        "\n",
        "\n",
        "print(\"Results for the pruned model:\")\n",
        "print(classification_report(Y_test, Y_pred_pruned))\n",
        "confusion_pruned = confusion_matrix(Y_test, Y_pred_pruned)\n",
        "print(f\"Confusion matrix for pruned model:\\n{confusion_pruned}\")\n",
        "print(f\"Pruned Model Average Inference Time: {end_time - start_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXwqJApoQ-VZ",
        "outputId": "023598f5-ffb1-4786-ecce-d75bb6269f00"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for the pruned model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        34\n",
            "           1       0.67      0.17      0.27        47\n",
            "           2       0.65      0.95      0.78       344\n",
            "           3       0.94      0.81      0.87       413\n",
            "           4       0.61      0.40      0.48       184\n",
            "           5       0.77      0.47      0.58       146\n",
            "           6       0.59      0.62      0.60       256\n",
            "           7       0.33      0.35      0.34        68\n",
            "           8       0.20      0.62      0.30        32\n",
            "\n",
            "    accuracy                           0.67      1524\n",
            "   macro avg       0.53      0.49      0.47      1524\n",
            "weighted avg       0.69      0.67      0.66      1524\n",
            "\n",
            "Confusion matrix for pruned model:\n",
            "[[  0   2   0   0   0   0   0  13  19]\n",
            " [  0   8   0   0   4   0   0  20  15]\n",
            " [  0   0 328   0   3   0  13   0   0]\n",
            " [  0   0   2 335   0  20  56   0   0]\n",
            " [  0   2  89   0  74   0  14   3   2]\n",
            " [  0   0   0  21  28  68  29   0   0]\n",
            " [  0   0  83   1  13   0 159   0   0]\n",
            " [  0   0   0   0   0   0   0  24  44]\n",
            " [  0   0   0   0   0   0   0  12  20]]\n",
            "Pruned Model Average Inference Time: 3.0003321170806885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, data):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(data)  # Perform inference\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time"
      ],
      "metadata": {
        "id": "IHLAib8jTOa2"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}