{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E8cHH8e9Cg1C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil  # https://docs.python.org/3/library/shutil.html\n",
        "from shutil import unpack_archive  # to unzip\n",
        "# from shutil import make_archive # to create zip for storage\n",
        "import requests  # for downloading zip file\n",
        "from scipy import io  # for loadmat, matlab conversion\n",
        "import numpy as np\n",
        "\n",
        "# credit https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
        "# many other methods I tried failed to download the file properly\n",
        "\n",
        "\n",
        "def download_url(url, save_path, chunk_size=128):\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(save_path, 'wb') as fd:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            fd.write(chunk)\n",
        "\n",
        "\n",
        "def unimib_load_dataset(\n",
        "        verbose=True,\n",
        "        incl_xyz_accel=False,  # include component accel_x/y/z in ____X data\n",
        "        # add rms value (total accel) of accel_x/y/z in ____X data\n",
        "        incl_rms_accel=True,\n",
        "        incl_val_group=False,  # True => returns x/y_test, x/y_validation, x/y_train\n",
        "    # False => combine test & validation groups\n",
        "        split_subj=dict\n",
        "    (train_subj=[4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 19, 20, 21, 22, 24, 26, 27, 29],\n",
        "     validation_subj=[1, 9, 16, 23, 25, 28],\n",
        "     test_subj=[2, 3, 13, 17, 18, 30]),\n",
        "        one_hot_encode=True):\n",
        "\n",
        "    # Download and unzip original dataset\n",
        "    if (not os.path.isfile('./UniMiB-SHAR.zip')):\n",
        "        print(\"Downloading UniMiB-SHAR.zip file\")\n",
        "        # invoking the shell command fails when exported to .py file\n",
        "        # redirect link https://www.dropbox.com/s/raw/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip\n",
        "        #!wget https://www.dropbox.com/s/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip\n",
        "        download_url(\n",
        "            'https://www.dropbox.com/s/raw/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip', './UniMiB-SHAR.zip')\n",
        "    if (not os.path.isdir('./UniMiB-SHAR')):\n",
        "        shutil.unpack_archive('./UniMiB-SHAR.zip', '.', 'zip')\n",
        "    # Convert .mat files to numpy ndarrays\n",
        "    path_in = './UniMiB-SHAR/data'\n",
        "    # loadmat loads matlab files as dictionary, keys: header, version, globals, data\n",
        "    adl_data = io.loadmat(path_in + '/adl_data.mat')['adl_data']\n",
        "    adl_names = io.loadmat(path_in + '/adl_names.mat',\n",
        "                           chars_as_strings=True)['adl_names']\n",
        "    adl_labels = io.loadmat(path_in + '/adl_labels.mat')['adl_labels']\n",
        "\n",
        "    # Reshape data and compute total (rms) acceleration\n",
        "    num_samples = 151\n",
        "    # UniMiB SHAR has fixed size of 453 which is 151 accelX, 151 accely, 151 accelz\n",
        "    adl_data = np.reshape(adl_data, (-1, num_samples, 3),\n",
        "                          order='F')  # uses Fortran order\n",
        "    if (incl_rms_accel):\n",
        "        rms_accel = np.sqrt(\n",
        "            (adl_data[:, :, 0]**2) + (adl_data[:, :, 1]**2) + (adl_data[:, :, 2]**2))\n",
        "        adl_data = np.dstack((adl_data, rms_accel))\n",
        "\n",
        "    # remove component accel if needed\n",
        "    if (not incl_xyz_accel):\n",
        "        adl_data = np.delete(adl_data, [0, 1, 2], 2)\n",
        "\n",
        "    # matlab source was 1 indexed, change to 0 indexed\n",
        "    act_num = (adl_labels[:, 0])-1\n",
        "    sub_num = (adl_labels[:, 1])  # subject numbers are in column 1 of labels\n",
        "\n",
        "    if (not incl_val_group):\n",
        "        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] +\n",
        "                                         split_subj['validation_subj']))\n",
        "        x_train = adl_data[train_index]\n",
        "        y_train = act_num[train_index]\n",
        "    else:\n",
        "        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj']))\n",
        "        x_train = adl_data[train_index]\n",
        "        y_train = act_num[train_index]\n",
        "\n",
        "        validation_index = np.nonzero(\n",
        "            np.isin(sub_num, split_subj['validation_subj']))\n",
        "        x_validation = adl_data[validation_index]\n",
        "        y_validation = act_num[validation_index]\n",
        "\n",
        "    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n",
        "    x_test = adl_data[test_index]\n",
        "    y_test = act_num[test_index]\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"x/y_train shape \", x_train.shape, y_train.shape)\n",
        "    if (incl_val_group):\n",
        "        print(\"x/y_validation shape \", x_validation.shape, y_validation.shape)\n",
        "    print(\"x/y_test shape  \", x_test.shape, y_test.shape)\n",
        "\n",
        "    if (incl_val_group):\n",
        "        return x_train, y_train, x_validation, y_validation, x_test, y_test\n",
        "    else:\n",
        "        return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYSBW_gKCnu4",
        "outputId": "374b491c-cd91-4146-c58e-5fd9f422267d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading UniMiB-SHAR.zip file\n",
            "x/y_train shape  (4601, 151, 4) (4601,)\n",
            "x/y_validation shape  (1454, 151, 4) (1454,)\n",
            "x/y_test shape   (1524, 151, 4) (1524,)\n",
            "Number of classes: 9\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_valid, y_valid, X_test, y_test = unimib_load_dataset(\n",
        "    incl_xyz_accel=True, incl_val_group=True, one_hot_encode=False, verbose=True\n",
        ")\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuuSAE4DC7iS",
        "outputId": "648a2bd8-870b-4374-8673-cdc711d1f1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mZrgnYcXCuk5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "import torch.quantization as quant\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4oKNfTFIDC4C"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "Y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "valid_dataset = TensorDataset(X_valid, Y_valid)\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, Y_test)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T6Job_-lDFDK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Calculate the number of patches, adjusting for padding if necessary\n",
        "        # Ceiling division to account for padding\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (\n",
        "            self.num_patches * patch_size\n",
        "        ) - input_timesteps  # Calculate padding length\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad the input sequence if necessary\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n",
        "\n",
        "        # We use a Conv1d layer to generate the patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfx_fefRDIxg",
        "outputId": "163e02cb-b6fc-48d8-8e17-9b77dfcfc12f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 151, 4]         [64, 20, 32]         32                   True\n",
              "├─Conv1d (conv_layer)                                             [64, 4, 152]         [64, 32, 19]         1,056                True\n",
              "├─PositionalEncoding (position_embeddings)                        [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "│    └─Dropout (dropout)                                          [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "=================================================================================================================================================\n",
              "Total params: 1,088\n",
              "Trainable params: 1,088\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.28\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.15\n",
              "Forward/backward pass size (MB): 0.31\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.47\n",
              "================================================================================================================================================="
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "summary(\n",
        "    model=patch_embedding_layer,\n",
        "    # (batch_size, input_channels, input_timesteps)\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jiv86iUbDL3e"
      },
      "outputs": [],
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        # Setting batch_first=True to accommodate inputs with batch dimension first\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Apply Transformer Encoder with batch first\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89MCYH4ZDOuT",
        "outputId": "945fc285-fc99-4324-dc10-147884553404"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "TimeSeriesTransformer (TimeSeriesTransformer)           [64, 151, 4]         [64, 9]              --                   True\n",
              "├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 151, 4]         [64, 20, 32]         32                   True\n",
              "│    └─Conv1d (conv_layer)                              [64, 4, 152]         [64, 32, 19]         1,056                True\n",
              "│    └─PositionalEncoding (position_embeddings)         [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "│    │    └─Dropout (dropout)                           [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "├─TransformerEncoder (transformer_encoder)              [64, 20, 32]         [64, 20, 32]         --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─TransformerEncoderLayer (0)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (1)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (2)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (3)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n",
              "├─Linear (classifier)                                   [64, 128]            [64, 9]              1,161                True\n",
              "=======================================================================================================================================\n",
              "Total params: 57,289\n",
              "Trainable params: 57,289\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.63\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.15\n",
              "Forward/backward pass size (MB): 0.38\n",
              "Params size (MB): 0.03\n",
              "Estimated Total Size (MB): 0.56\n",
              "======================================================================================================================================="
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymqOWnnbDQ3k",
        "outputId": "fdb1d615-0360-414c-ed1c-e7d5cc260ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.5163\n",
            "Epoch 1, Train Loss: 1.5847, Train Acc: 0.4087, Val Loss: 1.1184, Val Acc: 0.5163\n",
            "Epoch 2: New best model saved with validation accuracy: 0.6122\n",
            "Epoch 2, Train Loss: 1.0536, Train Acc: 0.5893, Val Loss: 0.9922, Val Acc: 0.6122\n",
            "Epoch 3: New best model saved with validation accuracy: 0.6591\n",
            "Epoch 3, Train Loss: 0.8321, Train Acc: 0.6659, Val Loss: 0.9185, Val Acc: 0.6591\n",
            "Epoch 4: New best model saved with validation accuracy: 0.6882\n",
            "Epoch 4, Train Loss: 0.7569, Train Acc: 0.6939, Val Loss: 0.8610, Val Acc: 0.6882\n",
            "Epoch 5: New best model saved with validation accuracy: 0.7010\n",
            "Epoch 5, Train Loss: 0.6742, Train Acc: 0.7324, Val Loss: 0.9232, Val Acc: 0.7010\n",
            "Epoch 6: New best model saved with validation accuracy: 0.7422\n",
            "Epoch 6, Train Loss: 0.6342, Train Acc: 0.7524, Val Loss: 0.7318, Val Acc: 0.7422\n",
            "Epoch 7, Train Loss: 0.5720, Train Acc: 0.7718, Val Loss: 0.7192, Val Acc: 0.7386\n",
            "Epoch 8, Train Loss: 0.5805, Train Acc: 0.7700, Val Loss: 0.8606, Val Acc: 0.7038\n",
            "Epoch 9: New best model saved with validation accuracy: 0.7649\n",
            "Epoch 9, Train Loss: 0.5232, Train Acc: 0.7978, Val Loss: 0.7505, Val Acc: 0.7649\n",
            "Epoch 10, Train Loss: 0.4866, Train Acc: 0.8129, Val Loss: 0.7823, Val Acc: 0.7493\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ],
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 10\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        predictions = model(inputs)  # Forward pass\n",
        "        loss = criterion(predictions, labels)  # Calculate loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Count the number of correct predictions\n",
        "        train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_acc = train_correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27-Xe3d2DdzD",
        "outputId": "fcfa9f71-a5df-49f1-acb2-452aaf25cc88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        34\n",
            "           1       0.31      0.66      0.42        47\n",
            "           2       0.68      0.98      0.80       344\n",
            "           3       1.00      0.82      0.90       413\n",
            "           4       0.46      0.33      0.39       184\n",
            "           5       0.96      0.95      0.95       146\n",
            "           6       0.61      0.46      0.52       256\n",
            "           7       0.53      0.82      0.64        68\n",
            "           8       0.36      0.12      0.19        32\n",
            "\n",
            "    accuracy                           0.71      1524\n",
            "   macro avg       0.54      0.57      0.53      1524\n",
            "weighted avg       0.71      0.71      0.70      1524\n",
            "\n",
            "Confusion matrix:\n",
            "[[  0  24   0   0   0   0   0  10   0]\n",
            " [  0  31   4   0   0   0   0  12   0]\n",
            " [  0   0 338   0   4   2   0   0   0]\n",
            " [  0   0   4 337   0   2  70   0   0]\n",
            " [  0  39  77   0  61   0   6   1   0]\n",
            " [  0   0   0   1   4 138   0   2   1]\n",
            " [  0   0  75   0  63   0 118   0   0]\n",
            " [  0   4   0   0   0   2   0  56   6]\n",
            " [  0   3   0   0   0   0   0  25   4]]\n",
            "Original Model Average Inference Time: 0.3195984363555908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "start_time = time.time()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "end_time = time.time()\n",
        "\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "confusion = confusion_matrix(Y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")\n",
        "print(f\"Original Model Average Inference Time: {end_time - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gkNMKGQ_DtXG"
      },
      "outputs": [],
      "source": [
        "# Define quantization configuration\n",
        "quant_config = quant.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Apply post-training static quantization\n",
        "quant_model = quant.quantize_dynamic(\n",
        "    model, qconfig_spec={\"\": quant_config}, dtype=torch.qint8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4dv31znEgrT",
        "outputId": "dfd9de4b-9567-4d4a-b2c7-f18883b9a4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.7294\n",
            "Epoch 1, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7859, Val Acc: 0.7294\n",
            "Epoch 2: New best model saved with validation accuracy: 0.7308\n",
            "Epoch 2, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7708, Val Acc: 0.7308\n",
            "Epoch 3, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7940, Val Acc: 0.7244\n",
            "Epoch 4, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7734, Val Acc: 0.7230\n",
            "Epoch 5, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7828, Val Acc: 0.7294\n",
            "Epoch 6, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7922, Val Acc: 0.7259\n",
            "Epoch 7, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7946, Val Acc: 0.7259\n",
            "Epoch 8: New best model saved with validation accuracy: 0.7337\n",
            "Epoch 8, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7676, Val Acc: 0.7337\n",
            "Epoch 9, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7770, Val Acc: 0.7259\n",
            "Epoch 10, Train Loss: 0.4896, Train Acc: 0.8066, Val Loss: 0.7803, Val Acc: 0.7251\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ],
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 10\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "# Validation loop\n",
        "    quant_model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nagjMYNGFea6",
        "outputId": "a089da14-ba07-4aa0-e94b-a789f0a804b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.38        34\n",
            "           1       0.15      0.11      0.12        47\n",
            "           2       0.57      0.95      0.71       344\n",
            "           3       0.98      0.84      0.90       413\n",
            "           4       0.54      0.27      0.36       184\n",
            "           5       0.97      0.92      0.95       146\n",
            "           6       0.52      0.34      0.41       256\n",
            "           7       0.47      0.88      0.62        68\n",
            "           8       0.36      0.12      0.19        32\n",
            "\n",
            "    accuracy                           0.67      1524\n",
            "   macro avg       0.55      0.53      0.52      1524\n",
            "weighted avg       0.68      0.67      0.65      1524\n",
            "\n",
            "Confusion matrix:\n",
            "[[ 12   4   0   0   1   0   0  17   0]\n",
            " [ 17   5   3   0   0   0   0  22   0]\n",
            " [  0   0 327   0   1   0  16   0   0]\n",
            " [  0   0   7 346   0   4  56   0   0]\n",
            " [  0  22 105   0  49   0   8   0   0]\n",
            " [  0   0   0   6   2 135   1   0   2]\n",
            " [  0   0 130   0  38   0  88   0   0]\n",
            " [  1   2   0   0   0   0   0  60   5]\n",
            " [  0   0   0   0   0   0   0  28   4]]\n",
            "Quantized Model Average Inference Time: 0.3708155155181885\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "start_time = time.time()\n",
        "quant_model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "end_time = time.time()\n",
        "\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "confusion = confusion_matrix(Y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")\n",
        "print(f\"Quantized Model Average Inference Time: {end_time - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ILMO0Kc8JDb7"
      },
      "outputs": [],
      "source": [
        "# Prune the model\n",
        "prune_amount = 0.2\n",
        "prune_method = prune.L1Unstructured\n",
        "\n",
        "# Get the sequence length from the shape of your input data\n",
        "sequence_length = X_train.shape[1]\n",
        "\n",
        "# Define dummy input shape based on your data\n",
        "batch_size = X_train.shape[0]\n",
        "input_dim = X_train.shape[2]\n",
        "input_shape = (batch_size, sequence_length, input_dim)\n",
        "\n",
        "# Define the parameters to be pruned\n",
        "parameters_to_prune = [\n",
        "    (name, module) for name, module in model.named_parameters() if 'your_module_to_prune' in name\n",
        "]\n",
        "\n",
        "# Apply pruning to model\n",
        "for name, module in parameters_to_prune:\n",
        "    prune_method(module, amount=prune_amount)  # Prune based on L1 norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hTZb7J5K9A-Z"
      },
      "outputs": [],
      "source": [
        "# Define the pruning method and amount\n",
        "import copy\n",
        "\n",
        "# Create a copy of the original model\n",
        "pruned_model = copy.deepcopy(model)\n",
        "\n",
        "# Prune the model\n",
        "prune_amount = 0.2\n",
        "prune_method = prune.L1Unstructured\n",
        "\n",
        "# Get the sequence length from the shape of your input data\n",
        "sequence_length = X_train.shape[1]\n",
        "\n",
        "# Define dummy input shape based on your data\n",
        "batch_size = X_train.shape[0]\n",
        "input_dim = X_train.shape[2]\n",
        "input_shape = (batch_size, sequence_length, input_dim)\n",
        "\n",
        "# Define the parameters to be pruned\n",
        "parameters_to_prune = [\n",
        "    (name, module) for name, module in model.named_parameters() if 'your_module_to_prune' in name\n",
        "]\n",
        "\n",
        "# Apply pruning to model\n",
        "for name, module in parameters_to_prune:\n",
        "    prune_method(module, amount=prune_amount)  # Prune based on L1 norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXwqJApoQ-VZ",
        "outputId": "340d50b8-d090-4f81-bab7-8b6985be5e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for the pruned model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        34\n",
            "           1       0.31      0.66      0.42        47\n",
            "           2       0.68      0.98      0.80       344\n",
            "           3       1.00      0.82      0.90       413\n",
            "           4       0.46      0.33      0.39       184\n",
            "           5       0.96      0.95      0.95       146\n",
            "           6       0.61      0.46      0.52       256\n",
            "           7       0.53      0.82      0.64        68\n",
            "           8       0.36      0.12      0.19        32\n",
            "\n",
            "    accuracy                           0.71      1524\n",
            "   macro avg       0.54      0.57      0.53      1524\n",
            "weighted avg       0.71      0.71      0.70      1524\n",
            "\n",
            "Confusion matrix for pruned model:\n",
            "[[  0  24   0   0   0   0   0  10   0]\n",
            " [  0  31   4   0   0   0   0  12   0]\n",
            " [  0   0 338   0   4   2   0   0   0]\n",
            " [  0   0   4 337   0   2  70   0   0]\n",
            " [  0  39  77   0  61   0   6   1   0]\n",
            " [  0   0   0   1   4 138   0   2   1]\n",
            " [  0   0  75   0  63   0 118   0   0]\n",
            " [  0   4   0   0   0   2   0  56   6]\n",
            " [  0   3   0   0   0   0   0  25   4]]\n",
            "Pruned Model Average Inference Time: 0.29556798934936523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the pruned model\n",
        "start_time = time.time()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob_pruned = pruned_model(X_test)\n",
        "\n",
        "Y_pred_pruned = Y_pred_prob_pruned.argmax(1)\n",
        "end_time = time.time()\n",
        "\n",
        "\n",
        "print(\"Results for the pruned model:\")\n",
        "print(classification_report(Y_test, Y_pred_pruned))\n",
        "confusion_pruned = confusion_matrix(Y_test, Y_pred_pruned)\n",
        "print(f\"Confusion matrix for pruned model:\\n{confusion_pruned}\")\n",
        "print(f\"Pruned Model Average Inference Time: {end_time - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtKBFNKkSW8i",
        "outputId": "e1fcdc37-2720-4391-e9fe-fb721ffee79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for the original model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.62      0.40        34\n",
            "           1       0.38      0.38      0.38        47\n",
            "           2       0.73      0.94      0.82       344\n",
            "           3       0.96      0.87      0.91       413\n",
            "           4       0.62      0.40      0.49       184\n",
            "           5       0.99      0.86      0.92       146\n",
            "           6       0.67      0.73      0.70       256\n",
            "           7       0.58      0.51      0.55        68\n",
            "           8       0.00      0.00      0.00        32\n",
            "\n",
            "    accuracy                           0.75      1524\n",
            "   macro avg       0.58      0.59      0.57      1524\n",
            "weighted avg       0.75      0.75      0.74      1524\n",
            "\n",
            "Confusion matrix for original model:\n",
            "[[ 21  12   0   0   0   0   0   1   0]\n",
            " [ 19  18   1   0   2   0   0   7   0]\n",
            " [  0   0 322   1   9   1  11   0   0]\n",
            " [  0   0   0 361   0   0  52   0   0]\n",
            " [  7   4  73   0  73   0  25   2   0]\n",
            " [  0   0   0  14   2 126   4   0   0]\n",
            " [  0   0  46   0  24   0 186   0   0]\n",
            " [ 14   7   0   1   7   0   0  35   4]\n",
            " [ 11   6   0   0   0   0   0  15   0]]\n"
          ]
        }
      ],
      "source": [
        "# Compare performance with the original model\n",
        "print(\"Results for the original model:\")\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "confusion_original = confusion_matrix(Y_test, Y_pred)\n",
        "print(f\"Confusion matrix for original model:\\n{confusion_original}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IdBzL367YiX",
        "outputId": "f77073e3-676b-4802-9f02-e21a2f425d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4601, 151, 4)\n"
          ]
        }
      ],
      "source": [
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IHLAib8jTOa2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, data):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(data)  # Perform inference\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
